# Bias-Variance Tradeoff

The bias-variance tradeoff is a central concept in machine learning that affects every model you train. High bias leads to underfitting, while high variance leads to overfitting. Understanding how to manage this tradeoff is key to building robust models.

---

## What You'll Learn

This folder contains theory and code covering:

1. **Foundations of Bias & Variance**
   - Visual intuition with polynomial regression
   - Mathematical definitions and decomposition of error
   - Classic U-shaped validation curve

2. **Regularization to Reduce Variance**
   - How Ridge and Lasso penalize model complexity
   - Visualizing coefficient shrinkage
   - Tradeoff between bias increase and variance reduction

3. **Bagging to Reduce Variance**
   - Bootstrap aggregating to stabilize high-variance models
   - Implementing Random Forests on noisy data
   - Visualizing decision boundaries before and after bagging

4. **Boosting to Reduce Bias**
   - Sequential learning to correct weak learners
   - Gradient Boosting and AdaBoost intuition
   - Visualizing how bias reduces across iterations

---

## Learning Resources

- [StatQuest: Bias and Variance](https://www.youtube.com/watch?v=EuBBz3bI-aA)
- [The Bias Variance Trade-Off](https://www.youtube.com/watch?v=FcXQKsZKRUs&t=46s)
